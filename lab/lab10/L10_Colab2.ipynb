{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://githubtocolab.com/Eunseob/purdue_me597/blob/main/lab/lab10/L10_Colab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"45O6i30z471K"}},{"cell_type":"markdown","source":["# Lab 10.2 Machine Learning 3 - Real-time Sound Recognition for Classification\n","\n","## CNN model deployment to Raspberry Pi\n"],"metadata":{"id":"sf3_litxLu9n"}},{"cell_type":"markdown","source":["## 2.1 TensorFlow Lite\n","\n","Based on the sample code in the previous section, let's apply the CNN model to predict the operational state of the vacuum pump. To do this, we will use TensorFlow Lite. TensorFlow Lite has limited features but is faster and lighter than TensorFlow. When you install TensorFlow on Raspberry Pi and laptop, TensorFlow Lite module is automatically installed. Because it supports Keras interface, TensorFlow Lite is able to load and run the CNN model developed in the prelab using TensorFlow and Keras.\n","\n","The best way to practice implementation of the trained model is to program yourself. Because of the limited time in the lab, however, the complete sample code ([lab10_sample2.py]()) is given. Perform Task 2.1 to interpret the model output as the appropriate label. Note that the CNN model (h5 format) must be in the same directory in which the sample code is.\n","\n","<br>"],"metadata":{"id":"muETKb5BDxxG"}},{"cell_type":"markdown","source":["### Task 2.1\n","\n","Run [lab10_sample2.py]() on your Raspberry Pi or laptop see if the CNN model works well. The terminal or output shell looks like as Figure 5. **Note that if you develop the CNN model with different feature size and shape, you have to modify the sample code.** An algorithm to interpret the CNN model output is incomplete as part of the sample code below. Complete the algorithm so that appropriate label is printed out at the end. To be specific, if Y (prediction inference index) is 0, the prediction label must be \"OFF\" (given) and if Y is 1, the prediction label must be \"ON-airleaking\", and so on.\n","\n","* Hint: Use if, elif, else statement.\n","* Hint: Another way is to use dictionary data structure with indexing.\n","\n","<br>\n","\n","---\n","\n","**<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/icon_Python.png?raw=tru\" width=\"20\">Python - Python3 (Last part of [lab10_sample2.py](https://github.com/Eunseob/purdue_me597/blob/main/lab/lab10/lab10_sample_code/lab10_sample1.py?raw=true))**\n","\n","```\n","91         if Y == 0: # if Y is 0, do below\n","92             prediction_label = \"OFF\"\n","93         # complete your algorithm to take appropriate prediction_label string according to the CNN model result\n","94 \n","95         print('The pump is now {}.\\n'.format(prediction_label))\n","96         \n","97         time.sleep(1) # pause for 1 second\n","\n","```\n","\n","---\n","\n","<br>\n","\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_fig5.png?raw=true\" width=\"90%\">\n","\n","*Figure 5 Terminal while running lab10_sample2.py*"],"metadata":{"id":"EBM_C9jijQKA"}},{"cell_type":"code","source":["## copy and paste your entire algorithm from line number 91 (if Y == 0: # if Y is 0, do below) to the end\n","\n"],"metadata":{"id":"sLn8NrRWpano"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Learning goals\n","\n","Students will be able to:\n","\n","1. Understand and utilize MTConnect sound data stream\n","2. Implement a CNN model to predict operational state of the vacuump pump\n","3. Build a real-time web-based monitoring system"],"metadata":{"id":"Ut8vxNTA_CJ3"}},{"cell_type":"markdown","source":["## 1.1 Introduction\n","\n","In this lab, we will implement real-time machine sound recognition using a CNN model and MTConnect sound stream to predict the operational state of the vacuum pump. This lab is broken down into three main sections: **1) Utilize MTConnect sound data stream**, **2) Implement a CNN model you developed in the prelab using TensorFlow Lite**, and **3) Build up the entire monitoring system.** \n","\n","To get started, Figure 1 illustrates a simplified sound recognition system. IIoT technologies and embedded machine learning models realize this system. The idea is that the Raspberry Pi listens to sound from the machine and recognize the operational state by AI model, and then we are able to monitor remotely the real-time state via a web-based dashboard. \n","\n","<br>\n","\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_fig1.png?raw=true\" width=\"70%\">\n","\n","*Figure 1 Simplified system configuration of vacuum pump sound recognition for Lab 10*\n","\n","<br>\n","\n","Details of the system configuration and data flow are shown in Figure 2. Please take a close look at Figure 2. Let's say the MTConnect stream for sound is \"sub-MTConnect\" for sound to distinguish it from the MTConnect for results. First, the internal sound sensor connected to the headless computer captures analog sound signals and then converts them to digital signals. Second, the sub-MTConnect adapter samples, formats, and converts them to the sub-MTConnect agent. Therefore, as long as a client is accessible via TCP/IP to the headless computer, it can read the sound stream using the sub-MTConnect agent in real-time. Third, the edge computer (Raspberry Pi) gets the sound stream and takes a signal processing for the input feature of the CNN model that we developed in Prelab 10. For AI implementation, we utilize TensorFlow Lite on Raspberry Pi. Fourth, the outputs of the CNN model with the AI algorithm are taken to MTConnect. Fifth, the MTConnect data collector program aggregates MTConnect agent data and sends them to the MySQL database. Lastly, Grafana dashboard with the MySQL database can let end users know the current/history running state of the vacuum pump.\n","\n","<br>\n","\n","\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_fig2.png?raw=true\" width=\"100%\">\n","\n","*Figure 2 Detailed system configuration and data flow for Lab 10*"],"metadata":{"id":"zMGw3LA8-5nN"}},{"cell_type":"markdown","source":["## 1.2 Sensor and server information\n","\n","Here, sensor specifications and server information (for MySQL and Grafana services) are summarized for your convenience, even though you are able to find the information in the previous prelab and lab manuals.\n","\n","Sensor specifications including the sub-MTconnect sound stream are in Table 1. [Sound pressure level](https://en.wikipedia.org/wiki/Sound_pressure) (dB SPL) is calculated by the sensitivity, SNR, gain of PCM (pulse-code modulation) in ALSA (advanced Linux sound architecture), and reference pressure (2×10<sup>-5</sup>  Pa). For the details of the conversion, please go to [reference 1](https://en.wikipedia.org/wiki/Decibel), and [reference 2](https://www.analog.com/en/analog-dialogue/articles/understanding-microphone-sensitivity.html#:~:text=The%20microphone's%20dynamic%20range%20is,a%2015%2Dbit%20data%20word.). The gain of PCM is +28.78 dB. Using the information of the sensor and sound stream, perform Task 1.1 to get access to the MTConnect sound stream.\n","\n","*Table 1 Sound sensor and stream specifications*\n","\n","<table>\n","<thead>\n","  <tr>\n","    <th>Target</th>\n","    <th>Specification</th>\n","    <th>Value</th>\n","    <th>Unit/remark</th>\n","  </tr>\n","</thead>\n","<tbody>\n","  <tr>\n","    <td rowspan=\"8\">USB Microphone</td>\n","    <td>Sensitivity</td>\n","    <td>-38±3</td>\n","    <td>dB</td>\n","  </tr>\n","  <tr>\n","    <td>Output impedance</td>\n","    <td>680</td>\n","    <td>Ω</td>\n","  </tr>\n","  <tr>\n","    <td>Directivity</td>\n","    <td>Unidirectional</td>\n","    <td>-</td>\n","  </tr>\n","  <tr>\n","    <td>Channel</td>\n","    <td>Mono</td>\n","    <td>-</td>\n","  </tr>\n","  <tr>\n","    <td>Frequency range</td>\n","    <td>20 - 24000</td>\n","    <td>Hz</td>\n","  </tr>\n","  <tr>\n","    <td>Operation voltage</td>\n","    <td>5</td>\n","    <td>V</td>\n","  </tr>\n","  <tr>\n","    <td>Current consumption</td>\n","    <td>0.5</td>\n","    <td>mA</td>\n","  </tr>\n","  <tr>\n","    <td>SNR (Signal to Noise Ratio)</td>\n","    <td>56</td>\n","    <td>dB</td>\n","  </tr>\n","  <tr>\n","    <td rowspan=\"8\">Sound stream</td>\n","    <td>Middleware</td>\n","    <td>MTConnect</td>\n","    <td></td>\n","  </tr>\n","  <tr>\n","    <td>Agent IP address</td>\n","    <td>192.168.1.4</td>\n","    <td>Router network (DNS: mepotrb16.ecn.purdue.edu)</td>\n","  </tr>\n","  <tr>\n","    <td>Port</td>\n","    <td>5001</td>\n","    <td></td>\n","  </tr>\n","  <tr>\n","    <td>Data type</td>\n","    <td>DiscreteTimeseries</td>\n","    <td></td>\n","  </tr>\n","  <tr>\n","    <td>Data format</td>\n","    <td>Signed 16-int</td>\n","    <td>Full scale is -1 to 1.</td>\n","  </tr>\n","  <tr>\n","    <td>Sampling frequency</td>\n","    <td>48</td>\n","    <td>kHz</td>\n","  </tr>\n","  <tr>\n","    <td>Count/sequence</td>\n","    <td>2048</td>\n","    <td>Approximately 43 msec</td>\n","  </tr>\n","  <tr>\n","    <td>dataitemId</td>\n","    <td>sensor1</td>\n","    <td></td>\n","  </tr>\n","</tbody>\n","</table>"],"metadata":{"id":"pUEdQUHpV_Cw"}},{"cell_type":"markdown","source":["### Task 1.1\n","\n","Using [curl](https://curl.se/) command below on Raspberry Pi and laptop, read current sound stream of the sensor and then attach it to the report as follows. Check the timestamp indicates current. It must be working on your laptop as well as long as both computers are on the same router network. Perform the same in the command prompt of your laptop as Raspberry Pi.\n","\n","\n","---\n","\n","**<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/icon_RaspberryPi.png?raw=tru\" width=\"20\">Raspberry Pi - Terminal AND <img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/icon_Windows.png?raw=tru\" width=\"20\"> Windows - Command Prompt**\n","\n","`curl 192.168.1.4:5001`\n","\n","OR\n","\n","`curl mepotrb16.ecn.purdue.edu:5001`\n","\n","---\n","\n","\n","\n","*   Note that the headless computer is connected to the router network (DNS: mepotrb16,ecn.purdue.edu). Because of the port forwarding of 5001 to the headless computer (192.168.1.4), we can has the same results if you get access both IP address and DNS.\n","\n","\n","\n","<br>\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_img1.png?raw=true\" width=\"100%\">\n","\n"],"metadata":{"id":"48RWsPyNhb38"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Please your capture here (Raspberry Pi)\n","\n","---\n","\n"],"metadata":{"id":"6LiTxjynijeL"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Please your capture here (Laptop)\n","\n","---\n"],"metadata":{"id":"VGSkys3ikFUV"}},{"cell_type":"markdown","source":["## 1.3 Handling MTConnect sound stream in Python\n","\n","Let's get access to the MTConnect sound stream and handle it using Python. Use [lab10_sample1.py](https://github.com/Eunseob/purdue_me597/blob/main/lab/lab10/lab10_sample_code/lab10_sample1.py?raw=true) which is almost complete other than some constant variables. In the Python script, We will read approximately 1-second-long data at the moment for the CNN model we developed in the prelab. And then, signal processing and sound pressure level calculation will be made. Eventually, we will check the sound signal by plotting them.\n","\n","The incomplete part of the sample script is below. You have to change the variables which is wrarpped by aserteric mark (*). By completing the sample code , perform Task 1.2."],"metadata":{"id":"C2Sf4TZloCjc"}},{"cell_type":"markdown","source":["---\n","\n","**<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/icon_Python.png?raw=tru\" width=\"20\">Python - Python3 ([lab10_sample1.py](https://github.com/Eunseob/purdue_me597/blob/main/lab/lab10/lab10_sample_code/lab10_sample1.py?raw=true))**\n","\n","```\n","## == CONSTANT ==\n","SAMPLE = \"sample\" # sample string for MTConnect HTML sample method\n","CURRENT = \"current\" # current string for MTConnect HTML current method\n","*SAMP_RATE* = int(?) # sampling rate\n","*CHUNK* = int(?) # chunk size\n","*AGENT* = \"http://?ip?:?port?/\" # MTConnect agent ip:port\n","\n","*N* = int(?) # number of sequence to take sound\n","*N_FFT* = int(?) # number of FFT\n","*N_MELS* = int(?) # number of Mel filter bank\n","\n","```\n"],"metadata":{"id":"lnHR1gpx56ri"}},{"cell_type":"markdown","source":["### Task 1.2\n","\n","To complete lab10_sample1.py, the required information is below. Note that you can use either Raspberry Pi or your laptop for this task.\n","\n","*   MTConnect (sound stream) agent IP: 192.168.1.4\n","*   MTConnect agent port number: 5001\n","*   Sampling frequency (sampling rate): MTConnect sound stream specification\n","*   Chunk size: MTConnect sound stream specification\n","*   Number of the required sequence: Same as CNN model training to make the sound signal approximately 1-second-long\n","*   Number of FFT data points: Same as the CNN model training\n","*   Number of Mel filter-bank: Same as the CNN model training\n","\n","<br>\n","\n","1. After completing it, plot the signal as Figure 3 and then attach it on the report.\n","\n","2. Calculate and print out the exact signal length in the second unit as Figure 4 and then attach the screenshot. You can use the commented-out line (line 57), but you have to replace \"?\" to get the answer.\n","\n","\n","<br>\n","\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_fig3.png?raw=true\" width=\"90%\">\n","\n","*Figure 3 Sound signal plot: time domain (left) and Mel-spectrogram (right)*\n","\n","<br>\n","\n","<img src=\"https://github.com/Eunseob/purdue_me597/blob/main/lab/img/lab10_fig4.png?raw=true\" width=\"80%\">\n","\n","*Figure 4 Terminal after running lab10_sample1.py*"],"metadata":{"id":"O_AP9gu02edU"}},{"cell_type":"markdown","source":["\n","\n","\n","---\n","\n","Place your screenshot for 1 of Task 1.2.\n","\n","---\n","\n","\n"],"metadata":{"id":"3k1Mc68_9k6t"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Place your screenshot for 2 of Task 1.2.\n","\n","---\n","\n"],"metadata":{"id":"2IN_WIOM9lQD"}},{"cell_type":"markdown","source":["<br></br>\n","\n","Please continue to [Lab 9.2 here](https://colab.research.google.com/github/Eunseob/purdue_me597/blob/main/lab/lab10/L10_Colab2.ipynb).\n"],"metadata":{"id":"B1wSJWp_OCF-"}}]}