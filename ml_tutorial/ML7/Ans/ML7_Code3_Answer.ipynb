{"cells":[{"cell_type":"markdown","metadata":{"id":"6ZFRFS7S0CZK"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRVTpWcA0CZL"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Import 'Tensorflow' pakage\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# Check the version of tensorflow\n","print(tf.__version__)"],"metadata":{"id":"rmqKmOnM0t83"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3v8xpJR0L_e"},"outputs":[],"source":["# Access to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"gxlBRHWY0CZL"},"source":["## Load the SELECTED (Top 30) Features Dataset\n","* Results of ML3-1 and ML3-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVQ2TSlK0CZM"},"outputs":[],"source":["FeatureSelected = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SavedFiles/FeatureSelected.csv', header=None)\n","FeatureSelected = FeatureSelected.T\n","FeatureSelected.shape"]},{"cell_type":"markdown","source":["## Standardize the feature values"],"metadata":{"id":"QP8ZbPqiXSCV"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","FeatureSelected_std = StandardScaler().fit_transform(FeatureSelected)\n","FeatureSelected_std.shape"],"metadata":{"id":"1zJYZ3hFZz7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zps3qPZH0CZN"},"source":["## Split Dataset into Training and Test Sets\n","- Use 'train_test_split' function\n","- It randomly samples the training and testing data according to the designated ratio."]},{"cell_type":"code","source":["# Number of data for each condition: 180\n","NoOfData   = int(FeatureSelected_std.shape[0]/2)\n","\n","# Separate the dataset into normal and abnormal sets\n","NormalSet   = FeatureSelected_std[:NoOfData , :]\n","AbnormalSet = FeatureSelected_std[NoOfData: , :]\n","\n","NormalSet.shape, AbnormalSet.shape"],"metadata":{"id":"sSHeYisJX8SZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection    import train_test_split\n","\n","# Define the test data ratio\n","TestData_Ratio = 0.2 \n","\n","# Split the normal and abnormal sets into training and test sets\n","TrainData_Nor, TestData_Nor = train_test_split(NormalSet  , test_size=TestData_Ratio, random_state=777)\n","TrainData_Abn, TestData_Abn = train_test_split(AbnormalSet, test_size=TestData_Ratio, random_state=777)\n","\n","print(TrainData_Nor.shape, TestData_Nor.shape)\n","print(TrainData_Abn.shape, TestData_Abn.shape)"],"metadata":{"id":"T_-DpjKpX7-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label the data (One-hot Encoding) using np.zeros and np.ones\n","- in this tutorial, [1,0] refers to 'Normal' and [1,0] refers to 'Abnormal'"],"metadata":{"id":"VlO_u2CjlvZY"}},{"cell_type":"code","source":["# Create labels for the training and test sets\n","TrainLabel_Nor = np.zeros((TrainData_Nor.shape[0],2))\n","TrainLabel_Abn = np.ones( (TrainData_Abn.shape[0],2)) \n","TestLabel_Nor  = np.zeros((TestData_Nor.shape[0],2))\n","TestLabel_Abn  = np.ones( (TestData_Abn.shape[0],2)) \n","\n","TrainLabel_Nor[:,0] = 1  # [1,0]: Normal\n","TrainLabel_Abn[:,0] = 0  # [0,1]: Abnormal\n","TestLabel_Nor[:,0]  = 1  # [1,0]: Normal\n","TestLabel_Abn[:,0]  = 0  # [0,1]: Abnormal\n","\n","print(TrainLabel_Nor.shape, TestLabel_Nor.shape)\n","print(TrainLabel_Abn.shape, TestLabel_Abn.shape)"],"metadata":{"id":"j4-2rzxQboWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the final Data and Label for ML modeling\n"],"metadata":{"id":"SR24Ptr1l9mY"}},{"cell_type":"code","source":["# Combine the normal and abnormal data/labels\n","TrainData  = np.concatenate([TrainData_Nor , TrainData_Abn ], axis=0)\n","TestData   = np.concatenate([TestData_Nor  , TestData_Abn  ], axis=0)\n","TrainLabel = np.concatenate([TrainLabel_Nor, TrainLabel_Abn], axis=0)\n","TestLabel  = np.concatenate([TestLabel_Nor , TestLabel_Abn ], axis=0)\n","\n","print(TrainData.shape,  TestData.shape)\n","print(TrainLabel.shape, TestLabel.shape)"],"metadata":{"id":"HnZh6FJKX7Bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n"],"metadata":{"id":"X0pJQJ3K1t8J"}},{"cell_type":"markdown","source":["## Grid search for Artificial Neural Network (ANN) hyperparameters"],"metadata":{"id":"IDkIDjOlmQXQ"}},{"cell_type":"markdown","source":["### [Main hyperparameters of ANN]\n","\n","1. **Number of hidden layers**: The number of hidden layers in an ANN determines the depth of the network. A deeper network can learn more complex patterns and representations of the data. However, increasing the number of hidden layers can also make the network more prone to overfitting and increase the computational cost.\n","\n",".\n","\n","2. **Number of neurons per hidden layer**:The number of neurons in each hidden layer determines the width of the network. A wider network can learn more complex representations of the data, but it also increases the number of trainable parameters and can lead to overfitting and increased computational cost.\n","\n",".\n","\n","3. **Activation functions**: Activation functions introduce non-linearity into the network, allowing it to learn complex patterns and representations. Common activation functions include:\n","\n","  - Sigmoid: $f(x) = \\frac{1}{1 + e^{-x}}$\n","  - Hyperbolic Tangent (tanh): $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n","  - Rectified Linear Unit (ReLU): $f(x) = max(0, x)$\n","  - Leaky ReLU: $f(x) = max(\\alpha x, x)$, where $\\alpha$ is a small constant (e.g., 0.01)\n","\n",".\n","\n","4. **Loss function**: The loss function measures the difference between the predicted output and the actual output (target) for each data point. The goal of training an ANN is to minimize the loss function. Common loss functions for classification tasks include:\n","\n","  - *(Categorical) Cross-Entropy Loss*: For multi-class classification problems.\n","  - *Binary Cross-Entropy Loss*: For binary classification problems.\n","\n",".\n","\n","5. **Optimizer**: The optimizer is an algorithm used to update the weights of the network during training to minimize the loss function. Common optimizers include:\n","\n","  - *Stochastic Gradient Descent (SGD)*: Simplest optimization algorithm, updates weights using a single data point, can be slow to converge.\n","  - *Momentum*: Extension of SGD with a momentum term, accelerates and dampens oscillations, converges faster.\n","  - *RMSProp*: Improvement over AdaGrad, resolves diminishing learning rate issue, suitable for non-stationary optimization problems.\n","  - *Adam*: Combines benefits of momentum and RMSProp, adapts learning rate for each weight, maintains smooth convergence, popular in deep learning.\n","\n",".\n","\n","6. **Learning rate**: The learning rate is a hyperparameter that controls the step size of the weight updates during training. A smaller learning rate will lead to slower convergence, while a larger learning rate may cause the model to overshoot the optimal weights and not converge at all.\n","\n",".\n","\n","7. **Epochs**: The number of epochs is the number of times the entire training dataset is passed through the network during training. Too few epochs can lead to underfitting, while too many epochs can lead to overfitting."],"metadata":{"id":"9LCD3vCifzhz"}},{"cell_type":"markdown","source":["### Prepare lists of hyperparameters for grid search"],"metadata":{"id":"EFAYWR27icNi"}},{"cell_type":"code","source":["# Hyperparameters for grid search\n","param_ActFn = ['tanh', 'relu'] # activation function\n","param_Layer = [2, 3]           # number of hiddent layers\n","param_Lrate = [0.001, 0.01]    # learning rate\n","\n","# Fixed hyperparameters\n","noOfNeuron = 16\n","Epoch      = 100\n","\n","# Calculate the number of cases\n","NoOfCases = len(param_ActFn) * len(param_Layer) * len(param_Lrate)\n","NoOfCases"],"metadata":{"id":"cPTQcGpPeTEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to create ANN models by inputting the hyperparameters for grid search\n","def ANN_model(input_data, noOfNeuron, temp_actfn, temp_layer, temp_lrate):\n","    keras.backend.clear_session()  # Clearing the Keras backend session (initiating variables)\n","\n","    model = keras.Sequential()\n","    model.add(keras.layers.InputLayer(input_shape=(input_data.shape[1],)))  # Input Layer\n","\n","    for i in range(temp_layer):\n","        model.add(keras.layers.Dense(units=noOfNeuron, activation=temp_actfn, name=f'Hidden{i+1}'))  # Hidden Layer\n","\n","    model.add(keras.layers.Dense(units=2, activation='softmax', name='Output'))  # Output Layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=temp_lrate),\n","                  loss=keras.losses.CategoricalCrossentropy(),\n","                  metrics=['accuracy'])\n","    return model"],"metadata":{"id":"Du2u1BYrfPxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an empty dataframe to store the accuracy results\n","Accuracy_df = pd.DataFrame(np.zeros(shape=(NoOfCases , 4)),\n","                           columns=['kernel', 'C', 'gamma', 'Accuracy'])\n","Accuracy_df"],"metadata":{"id":"BrPkRQ5riyzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the ANN models with different combinations of hyperparameters and save them"],"metadata":{"id":"ZtUV-NiYZMVH"}},{"cell_type":"code","source":["# Initialize a count value to store the performance of each model\n","cnt = 0\n","\n","# Iterate through all possible combinations of activation functions, hidden layers, and learning rates\n","for temp_actfn in param_ActFn:          # Select each activation function in the list\n","    for temp_layer in param_Layer:      # Select each hidden layer configuration in the list\n","        for temp_lrate in param_Lrate:  # Select each learning rate value in the list\n","            \n","            # Create, train, and validate a temporary ANN model with the current combination of hyperparameters\n","            temp_ann_model = ANN_model(TrainData, noOfNeuron, temp_actfn, temp_layer, temp_lrate)\n","            temp_ann_model.fit(TrainData, TrainLabel, epochs=Epoch, verbose=0)\n","            Loss, Accuracy = temp_ann_model.evaluate(TestData,  TestLabel, verbose=0)\n","\n","            # Save the temporary model to a file with a corresponding name\n","            temp_ann_model_name = f'ANN_{temp_actfn}_L{temp_layer}_LR{temp_lrate:.4f}.h5'\n","            temp_ann_model.save('/content/drive/MyDrive/Colab Notebooks/SavedFiles/ML_Models/GridSearch_ANN/' + temp_ann_model_name)\n","            \n","            # Store the performance (accuracy) of the temporary model in the dataframe\n","            Accuracy_df.iloc[cnt, :] = [temp_actfn, temp_layer, temp_lrate, Accuracy]\n","            cnt += 1\n","\n","# Display the resulting dataframe with model performances\n","Accuracy_df"],"metadata":{"id":"66ethux5Jw-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confirm the grid search results"],"metadata":{"id":"jCTJxmOuboMM"}},{"cell_type":"code","source":["# Sort the Accuracy_df by 'Accuracy' column in descending order\n","Accuracy_df_sorted = Accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n","\n","# Output the best case\n","print(\"[Best case]\\nActivation Function: \" + Accuracy_df_sorted.iloc[0, 0] +\n","      \"\\nHidden Layers: %d\\nLearning Rate: %.4f\\n\\nAccuracy: %.2f\" % (Accuracy_df_sorted.iloc[0, 1],\n","                                                                       Accuracy_df_sorted.iloc[0, 2],\n","                                                                       Accuracy_df_sorted.iloc[0, 3]))"],"metadata":{"id":"zxNrgSgLkycD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation accuracy for each activation function\n","mean_accuracy_ActFn = Accuracy_df.groupby(['kernel'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_ActFn"],"metadata":{"id":"ndmF-k2NSWvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation of accuracy for each hidden layer\n","mean_accuracy_Layer = Accuracy_df.groupby(['C'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_Layer"],"metadata":{"id":"cA63DIElTXsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation of accuracy for each learning rate\n","mean_accuracy_Lrate = Accuracy_df.groupby(['gamma'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_Lrate"],"metadata":{"id":"s8ueF8DETYSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize the performance comparison for the selected hyperparameter"],"metadata":{"id":"Sawr7_1-biqV"}},{"cell_type":"code","source":["# Set an index to select a hyperparmeter\n","# 0: activation function // 1: hidden layers // 2: learning rate\n","idx = 1\n","\n","# Automatically define variables based on the selected index\n","H_Param = ['ActFn', 'Layer', 'Lrate']\n","H_Param_name = ['Activation Function', 'Hidden Layers', 'Learning Rate']\n","Selected = H_Param[idx]\n","Selected_name = H_Param_name[idx]\n","exec('Result = mean_accuracy_' + H_Param[idx])\n","\n","xLabel = Result.iloc[:, 0]\n","x_pos = np.arange(Result.shape[0])\n","y_val = Result['mean']\n","y_err = Result['std']\n","\n","# Draw a bar chart to compare the model performance (diagnostic accuracy) for each hyperparameter\n","fig, ax = plt.subplots(figsize=(10, 5))\n","\n","# Create a bar plot with error bars\n","ax.bar(x_pos, y_val, yerr=y_err, align='center', alpha=0.5, ecolor='black', capsize=10,\n","       color=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple'])\n","ax.set_ylabel('Accuracy (mean)', fontsize=15)\n","ax.set_title(f\"Model performance comparsion by '{Selected_name}'\", fontsize=20)\n","ax.set_xticks(x_pos)\n","ax.set_xticklabels(xLabel, fontsize=15)\n","ax.yaxis.grid()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"-JZ0deF1Ti3z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confusion matrix and evaluation metrics for the best ANN model"],"metadata":{"id":"kb2KMKULjJm-"}},{"cell_type":"code","source":["# Retrieve activation function, hidden layers, and learning rate values from the first row of 'Accuracy_df_sorted'\n","Best_ActFn = Accuracy_df_sorted.iloc[0, 0]\n","Best_Layer = int(Accuracy_df_sorted.iloc[0, 1])\n","Best_Lrate = Accuracy_df_sorted.iloc[0, 2]\n","\n","# Load the best ANN model using the retrieved hyperparameters\n","best_ann_model_name = f'ANN_{Best_ActFn}_L{Best_Layer}_LR{Best_Lrate:.4f}.h5'\n","best_ann_model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/SavedFiles/ML_Models/GridSearch_ANN/' + best_ann_model_name)\n","\n","# Predict the output (Robotic spot-welding condition) for the test data\n","Predicted = best_ann_model.predict(TestData)\n","\n","# Convert TestLabel and Predicted into vectors to calculate the confusion matrix and evaluation metrics\n","TestLabel_rev = np.argmax(TestLabel, axis=1)\n","Predicted_rev = np.argmax(Predicted, axis=1)\n","\n","# Plot the confusion matrix\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(TestLabel_rev, Predicted_rev)\n","\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, square=True)\n","plt.xlabel(\"Predicted label\")\n","plt.ylabel(\"True label\")\n","plt.title(\"Confusion Matrix of the Best ANN Model\")\n","plt.show()\n","\n","from sklearn import metrics\n","\n","# Calculate the evaluation metrics\n","accuracy  = metrics.accuracy_score(TestLabel_rev, Predicted_rev)\n","precision = metrics.precision_score(TestLabel_rev, Predicted_rev)\n","recall    = metrics.recall_score(TestLabel_rev, Predicted_rev)\n","f1_score  = metrics.f1_score(TestLabel_rev, Predicted_rev)\n","\n","# Print the evaluation metrics\n","print(\"\\n\\n\")\n","print(f\"Best ANN Model Evaluation:\\n\")\n","print(f\"Accuracy : {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall   : {recall:.2f}\")\n","print(f\"F1 Score : {f1_score:.2f}\")"],"metadata":{"id":"PDU5uRTf3LTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LLqZbX-p9NkG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WsGTttiM2y9pNEFCnP-bbBr-BTuFEtk8","timestamp":1679089622944}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}