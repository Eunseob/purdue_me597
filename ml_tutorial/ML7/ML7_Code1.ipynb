{"cells":[{"cell_type":"markdown","metadata":{"id":"6ZFRFS7S0CZK"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRVTpWcA0CZL"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3v8xpJR0L_e"},"outputs":[],"source":["# Access to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"gxlBRHWY0CZL"},"source":["## Load the SELECTED (Top 30) Features Dataset\n","* Results of ML3-1 and ML3-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVQ2TSlK0CZM"},"outputs":[],"source":["FeatureSelected = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SavedFiles/FeatureSelected.csv', header=None)\n","FeatureSelected = FeatureSelected.T\n","FeatureSelected.shape"]},{"cell_type":"markdown","source":["## Standardize the feature values"],"metadata":{"id":"QP8ZbPqiXSCV"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","FeatureSelected_std = StandardScaler().fit_transform(FeatureSelected)\n","FeatureSelected_std.shape"],"metadata":{"id":"1zJYZ3hFZz7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zps3qPZH0CZN"},"source":["## Split Dataset into Training and Test Sets\n","- Use 'train_test_split' function\n","- It randomly samples the training and testing data according to the designated ratio."]},{"cell_type":"code","source":["# Number of data for each condition: 180\n","NoOfData   = int(FeatureSelected_std.shape[0]/2)\n","\n","# Separate the dataset into normal and abnormal sets\n","NormalSet   = FeatureSelected_std[:NoOfData , :]\n","AbnormalSet = FeatureSelected_std[NoOfData: , :]\n","\n","NormalSet.shape, AbnormalSet.shape"],"metadata":{"id":"sSHeYisJX8SZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection    import train_test_split\n","\n","# Define the test data ratio\n","TestData_Ratio = 0.2 \n","\n","# Split the normal and abnormal sets into training and test sets\n","TrainData_Nor, TestData_Nor = train_test_split(NormalSet  , test_size=TestData_Ratio, random_state=777)\n","TrainData_Abn, TestData_Abn = train_test_split(AbnormalSet, test_size=TestData_Ratio, random_state=777)\n","\n","print(TrainData_Nor.shape, TestData_Nor.shape)\n","print(TrainData_Abn.shape, TestData_Abn.shape)"],"metadata":{"id":"T_-DpjKpX7-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label the data using np.zeros and np.ones\n","- in this tutorial, 0 refers to 'Normal' and 1 refers to 'Abnormal'"],"metadata":{"id":"VlO_u2CjlvZY"}},{"cell_type":"code","source":["# Create labels for the training and test sets\n","TrainLabel_Nor = np.zeros(TrainData_Nor.shape[0]) # 0: Normal\n","TrainLabel_Abn = np.ones( TrainData_Abn.shape[0]) # 1: Abnormal\n","TestLabel_Nor  = np.zeros(TestData_Nor.shape[0])  # 0: Normal\n","TestLabel_Abn  = np.ones( TestData_Abn.shape[0])  # 1: Abnormal\n","\n","print(TrainLabel_Nor.shape, TestLabel_Nor.shape)\n","print(TrainLabel_Abn.shape, TestLabel_Abn.shape)"],"metadata":{"id":"j4-2rzxQboWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the final Data and Label for ML modeling\n"],"metadata":{"id":"SR24Ptr1l9mY"}},{"cell_type":"code","source":["# Combine the normal and abnormal data/labels\n","TrainData  = np.concatenate([TrainData_Nor , TrainData_Abn ], axis=0)\n","TestData   = np.concatenate([TestData_Nor  , TestData_Abn  ], axis=0)\n","TrainLabel = np.concatenate([TrainLabel_Nor, TrainLabel_Abn], axis=0)\n","TestLabel  = np.concatenate([TestLabel_Nor , TestLabel_Abn ], axis=0)\n","\n","print(TrainData.shape,  TestData.shape)\n","print(TrainLabel.shape, TestLabel.shape)"],"metadata":{"id":"HnZh6FJKX7Bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n"],"metadata":{"id":"NrNZ0sGP1n9i"}},{"cell_type":"markdown","source":["## Grid search for Support Vector Machine (SVM) hyperparameters"],"metadata":{"id":"IDkIDjOlmQXQ"}},{"cell_type":"markdown","source":["### [Main hyperparameters of SVM]\n","\n","1. **Kernel type**: The kernel is a function that transforms the input data into a higher-dimensional space, making it easier to find the optimal decision boundary. There are several kernel types available for SVM, but the most common ones are:\n","\n","- Linear: $K(x, y) = x^T * y$\n","- Polynomial: $K(x, y) = (gamma * x^T * y + coef0)^{degree}$\n","- Radial basis function (RBF): $K(x, y) = exp(-gamma * ||x - y||^2)$\n","- Sigmoid: $K(x, y) = tanh(gamma * x^T * y + coef0)$\n","\n",".\n","\n","2. **$C$ (Cost parameter)**: $C$ is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error. A smaller value of $C$ creates a wider margin but allows some misclassifications, which can be useful for noisy data. A larger value of $C$ will force the SVM to classify all training samples correctly, leading to a smaller margin and potentially overfitting. Choosing the appropriate value for $C$ is critical for the model's performance.\n","\n",".\n","\n","3. **$Gamma (Î³)$**: Gamma is a parameter specific to the RBF and polynomial kernels. It controls the shape of the decision boundary. A small gamma value results in a more flexible decision boundary, while a large gamma value leads to a more rigid decision boundary. Selecting the right gamma value is essential for avoiding overfitting or underfitting."],"metadata":{"id":"9LCD3vCifzhz"}},{"cell_type":"markdown","source":["### Prepare lists of hyperparameters for grid search"],"metadata":{"id":"EFAYWR27icNi"}},{"cell_type":"code","source":["param_kernel = ['linear', 'poly', 'rbf', 'sigmoid'] # kernel type\n","param_C      = [0.01, 0.1, 1, 10, 100]              # regularization parameter\n","param_gamma  = [0.01, 0.1, 1, 10, 100]              # boundary shape parameter\n","\n","# Calculate the number of cases\n","NoOfCases = len(param_kernel) * len(param_C) * len(param_gamma)\n","NoOfCases"],"metadata":{"id":"cPTQcGpPeTEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an empty dataframe to store the accuracy results\n","Accuracy_df = pd.DataFrame(np.zeros(shape=(NoOfCases , 4)),\n","                           columns=['kernel', 'C', 'gamma', 'Accuracy'])\n","Accuracy_df"],"metadata":{"id":"BrPkRQ5riyzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the SVM models with different combinations of hyperparameters and save them"],"metadata":{"id":"ZtUV-NiYZMVH"}},{"cell_type":"code","source":["# Import necessary packages for SVM\n","from sklearn import svm\n","import joblib\n","\n","# Initialize a count value to store the performance of each model\n","cnt = 0\n","\n","# Iterate through all possible combinations of kernel, C, and gamma values\n","for temp_kernel in param_kernel:            # Select each 'kernel' type in the list\n","    for temp_c in param_C:                  # Select each 'C' (regularization parameter) value in the list\n","        for temp_gamma in param_gamma:      # Select each 'gamma' (boundary shape parameter) value in the list\n","            \n","            # Create, train, and validate a temporary SVM model with the current combination of hyperparameters\n","            tempsvmModel = svm.SVC(kernel=temp_kernel, C=temp_c, gamma=temp_gamma)\n","            tempsvmModel.fit(TrainData, TrainLabel)\n","            tempAccuracy = tempsvmModel.score(TestData, TestLabel)\n","\n","            # Save the temporary model to a file with a corresponding name\n","            formatted_C     = f\"{temp_c:.2f}\"     # Format the C values with two decimal places\n","            formatted_gamma = f\"{temp_gamma:.2f}\" # Format the gamma values with two decimal places\n","            tempsvmModel_name = f'SVM_{temp_kernel}_C{formatted_C}_G{formatted_gamma}.plk'\n","            joblib.dump(tempsvmModel, '/content/drive/MyDrive/Colab Notebooks/SavedFiles/ML_Models/GridSearch_SVM/'+tempsvmModel_name)\n","            \n","            # Store the performance (accuracy) of the temporary model in the dataframe\n","            Accuracy_df.iloc[cnt, :] = [temp_kernel, temp_c, temp_gamma, tempAccuracy]\n","            cnt += 1\n","\n","# Display the resulting dataframe with model performances\n","Accuracy_df"],"metadata":{"id":"Du2u1BYrfPxW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confirm the grid search results"],"metadata":{"id":"jCTJxmOuboMM"}},{"cell_type":"code","source":["# Sort the Accuracy_df by 'Accuracy' column in descending order\n","Accuracy_df_sorted = Accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n","\n","# Output the best case\n","print(\"[Best case]\\nKernel: \" +Accuracy_df_sorted.iloc[0,0]+\n","      \"\\nC     : %.2f\\ngamma : %.2f\\n\\nAccuracy: %.2f\"%(Accuracy_df_sorted.iloc[0,1], \n","                                                        Accuracy_df_sorted.iloc[0,2], \n","                                                        Accuracy_df_sorted.iloc[0,3]))"],"metadata":{"id":"zxNrgSgLkycD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation accuracy for each kernel\n","mean_accuracy_Kernel = Accuracy_df.groupby(['kernel'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_Kernel"],"metadata":{"id":"ndmF-k2NSWvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation of accuracy for each C\n","mean_accuracy_C = Accuracy_df.groupby(['C'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_C"],"metadata":{"id":"cA63DIElTXsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean and standard deviation of accuracy for each gamma\n","mean_accuracy_Gamma = Accuracy_df.groupby(['gamma'])['Accuracy'].agg(['mean', 'std']).reset_index()\n","mean_accuracy_Gamma"],"metadata":{"id":"s8ueF8DETYSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize the performance comparison for the selected hyperparameter"],"metadata":{"id":"Sawr7_1-biqV"}},{"cell_type":"code","source":["# Set an index to select a hyperparmeter\n","# 0: kernel // 1: C // 2: Gamma\n","idx = 0\n","\n","# Automatically define variables based on the selected index\n","H_Param  = ['Kernel', 'C', 'Gamma']\n","Selected = H_Param[idx]\n","exec('Result = mean_accuracy_' + H_Param[idx])\n","\n","xLabel = Result.iloc[:,0]\n","x_pos = np.arange(Result.shape[0])\n","y_val = Result['mean']\n","y_err = Result['std']\n","\n","# Draw a bar chart to compare the model performance (diagnostic accuracy) for each hyperparameter\n","fig, ax = plt.subplots(figsize=(10,5))\n","\n","# Create a bar plot with error bars\n","ax.bar(x_pos, y_val, yerr=y_err, align='center', alpha=0.5, ecolor='black', capsize=10,\n","       color = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple'])\n","ax.set_ylabel('Accuracy (mean)', fontsize=15)\n","ax.set_title(f\"Model performance comparsion by '{Selected}'\", fontsize=20)\n","ax.set_xticks(x_pos)\n","ax.set_xticklabels(xLabel, fontsize=15)\n","ax.yaxis.grid()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"-JZ0deF1Ti3z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n","."],"metadata":{"id":"kb2KMKULjJm-"}},{"cell_type":"markdown","source":["# Task\n","\n","- Load the 90th ranked SVM model and predict the output (Robotic spot-welding condition) for test data.\n","\n","1. Refer to the provided codes if you need assistance.\n","2. Regard the 90th row of 'Accuracy_df_sorted' as the 90th ranked case.\n","3. Follow these steps:\n","    - Retrieve the kernel, C, and gamma values from the 'Accuracy_df_sorted'.\n","    - Format the C and gamma values with two decimal places.\n","    - Load the 90th ranked SVM model using the retrieved hyperparameters.\n","    - Predict the output (Robotic spot-welding condition) for the test data."],"metadata":{"id":"U9iQnVHLqhw9"}},{"cell_type":"code","source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","Predicted = "],"metadata":{"id":"PDU5uRTf3LTj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- You can confirm the performance of model by 1) confusion matrix and 2) evaluation metrics "],"metadata":{"id":"Fwkw7CM_oqUR"}},{"cell_type":"markdown","source":["\n","## [ Confusion Matrix ]\n","- A table that visualizes the performance of a classification model by displaying the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. The rows represent the true class labels, while the columns represent the predicted class labels. In a binary classification problem:\n","\n","    - TP: The number of instances where the model correctly predicted the positive class.\n","    - TN: The number of instances where the model correctly predicted the negative class.\n","    - FP: The number of instances where the model falsely predicted the positive class (actual negative instances).\n","    - FN: The number of instances where the model falsely predicted the negative class (actual positive instances)."],"metadata":{"id":"cXd65y16npwx"}},{"cell_type":"code","source":["# Plot the confusion matrix\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(TestLabel, Predicted)\n","\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, square=True)\n","plt.xlabel(\"Predicted label\")\n","plt.ylabel(\"True label\")\n","plt.title(\"Confusion Matrix of the Best SVM Model\")\n","plt.show()"],"metadata":{"id":"kIr428mHcdhT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [ Evaluation metrics (for classfication) ]\n","\n","1. $Accuracy$: The proportion of correctly classified instances out of the total instances. It measures the overall performance of a classification model.\n","\n","    - $Accuracy: (TP + TN) / (TP + TN + FP + FN)$\n","\n","2. $Precision$: The proportion of true positive instances among the instances predicted as positive. It measures how well the model correctly identifies positive instances.\n","\n","    - $Precision: TP / (TP + FP)$\n","\n","3. $Recall$: The proportion of true positive instances among the actual positive instances. It measures the ability of the model to find all the positive instances.\n","\n","    - $Recall: TP / (TP + FN)$\n","\n","4. $F1 Score$: The harmonic mean of precision and recall. It provides a single score that balances both precision and recall, which is especially useful when dealing with imbalanced datasets.\n","\n","    - $F1 Score: 2 * (Precision * Recall) / (Precision + Recall)$"],"metadata":{"id":"PsICHq_XocY5"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","# Calculate the evaluation metrics\n","accuracy  = metrics.accuracy_score(TestLabel, Predicted)\n","precision = metrics.precision_score(TestLabel, Predicted)\n","recall    = metrics.recall_score(TestLabel, Predicted)\n","f1_score  = metrics.f1_score(TestLabel, Predicted)\n","\n","# Print the evaluation metrics\n","print(f\"Best SVM Model Evaluation:\\n\")\n","print(f\"Accuracy : {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall   : {recall:.2f}\")\n","print(f\"F1 Score : {f1_score:.2f}\")"],"metadata":{"id":"6yCkG_IZobY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"biXOWTBJlo2l"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WsGTttiM2y9pNEFCnP-bbBr-BTuFEtk8","timestamp":1679089622944}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}