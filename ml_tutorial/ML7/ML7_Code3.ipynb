{"cells":[{"cell_type":"markdown","metadata":{"id":"6ZFRFS7S0CZK"},"source":["# Task\n","\n","1. Import necessary packages, including TensorFlow, and access your Google Drive.\n","2. Load the top 30 selected features and standardize them.\n","3. Prepare the training and test datasets, along with their labels, for ANN training.\n","4. Perform a grid search for ANN hyperparameters:\n","  - In this tutorial, the target hyperparameters are 'activation functions', 'number of hidden layers', and 'learning rate'.\n","  - Set up to 20 different hyperparameter combinations.\n","  - Train each model with up to 100 epochs.\n","  - Use up to 30 neurons for each hidden layer.\n","  - Create temporary ANN models within the loop using the provided '*ANN_model*' function.\n","  - Store the performance (diagnostic accuracy) of each temporary model in a DataFrame (Accuracy_df)\n","5. Evaluate the best ANN model based on the confusion matrix and other evaluation metrics:\n","  - Consider the first row of the sorted 'Accuracy_df' as the best case.\n","  - Load and use the best model to make predictions on the test dataset.\n","  - Calculate and display the confusion matrix and evaluation metrics (accuracy, precision, recall, and F1 score) for the best model.\n","\n",".\n","\n","- *Refer to ML6_Code1 and ML7_Code1*"]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n","."],"metadata":{"id":"NwNAykScNb8p"}},{"cell_type":"markdown","source":["Prepare Data and Labels for ANN"],"metadata":{"id":"4PFWgAoMNfY1"}},{"cell_type":"code","source":[],"metadata":{"id":"HnZh6FJKX7Bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n",".\n","\n"],"metadata":{"id":"X0pJQJ3K1t8J"}},{"cell_type":"markdown","source":["## Grid search for Artificial Neural Network (ANN) hyperparameters"],"metadata":{"id":"IDkIDjOlmQXQ"}},{"cell_type":"markdown","source":["### [Main hyperparameters of ANN]\n","\n","1. **Number of hidden layers**: The number of hidden layers in an ANN determines the depth of the network. A deeper network can learn more complex patterns and representations of the data. However, increasing the number of hidden layers can also make the network more prone to overfitting and increase the computational cost.\n","\n",".\n","\n","2. **Number of neurons per hidden layer**:The number of neurons in each hidden layer determines the width of the network. A wider network can learn more complex representations of the data, but it also increases the number of trainable parameters and can lead to overfitting and increased computational cost.\n","\n",".\n","\n","3. **Activation functions**: Activation functions introduce non-linearity into the network, allowing it to learn complex patterns and representations. Common activation functions include:\n","\n","  - Sigmoid: $f(x) = \\frac{1}{1 + e^{-x}}$\n","  - Hyperbolic Tangent (tanh): $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n","  - Rectified Linear Unit (ReLU): $f(x) = max(0, x)$\n","  - Leaky ReLU: $f(x) = max(\\alpha x, x)$, where $\\alpha$ is a small constant (e.g., 0.01)\n","\n",".\n","\n","4. **Loss function**: The loss function measures the difference between the predicted output and the actual output (target) for each data point. The goal of training an ANN is to minimize the loss function. Common loss functions for classification tasks include:\n","\n","  - *(Categorical) Cross-Entropy Loss*: For multi-class classification problems.\n","  - *Binary Cross-Entropy Loss*: For binary classification problems.\n","\n",".\n","\n","5. **Optimizer**: The optimizer is an algorithm used to update the weights of the network during training to minimize the loss function. Common optimizers include:\n","\n","  - *Stochastic Gradient Descent (SGD)*: Simplest optimization algorithm, updates weights using a single data point, can be slow to converge.\n","  - *Momentum*: Extension of SGD with a momentum term, accelerates and dampens oscillations, converges faster.\n","  - *RMSProp*: Improvement over AdaGrad, resolves diminishing learning rate issue, suitable for non-stationary optimization problems.\n","  - *Adam*: Combines benefits of momentum and RMSProp, adapts learning rate for each weight, maintains smooth convergence, popular in deep learning.\n","\n",".\n","\n","6. **Learning rate**: The learning rate is a hyperparameter that controls the step size of the weight updates during training. A smaller learning rate will lead to slower convergence, while a larger learning rate may cause the model to overshoot the optimal weights and not converge at all.\n","\n",".\n","\n","7. **Epochs**: The number of epochs is the number of times the entire training dataset is passed through the network during training. Too few epochs can lead to underfitting, while too many epochs can lead to overfitting."],"metadata":{"id":"9LCD3vCifzhz"}},{"cell_type":"markdown","source":["### Prepare lists of hyperparameters for grid search"],"metadata":{"id":"EFAYWR27icNi"}},{"cell_type":"code","source":["# Hyperparameters for grid search\n","param_ActFn = [] # activation function      (e.g., ['relu', 'tanh', 'sigmoid'])\n","                 # you can just put names of activation functions\n","                 # Refer to names: https://keras.io/api/layers/activations/\n","param_Layer = [] # number of hiddent layers (e.g., [2, 3, 5, 10])\n","param_Lrate = [] # learning rate            (e.g., [0.0001, 0.001, 0.01])\n","\n","# Fixed hyperparameters\n","noOfNeuron = \n","Epoch      = "],"metadata":{"id":"cPTQcGpPeTEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to create ANN models by inputting the hyperparameters for grid search\n","\n","def ANN_model(input_data, noOfNeuron, temp_actfn, temp_layer, temp_lrate):\n","    keras.backend.clear_session()  # Clearing the Keras backend session (initiating variables)\n","\n","    model = keras.Sequential()\n","    model.add(keras.layers.InputLayer(input_shape=(input_data.shape[1],)))  # Input Layer\n","\n","    for i in range(temp_layer):\n","        model.add(keras.layers.Dense(units=noOfNeuron, activation=temp_actfn, name=f'Hidden{i+1}'))  # Hidden Layer\n","\n","    model.add(keras.layers.Dense(units=2, activation='softmax', name='Output'))  # Output Layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=temp_lrate),\n","                  loss=keras.losses.CategoricalCrossentropy(),\n","                  metrics=['accuracy'])\n","    return model"],"metadata":{"id":"GIUEk0bxMUde"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the ANN models with different combinations of hyperparameters and save them"],"metadata":{"id":"ZtUV-NiYZMVH"}},{"cell_type":"code","source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","Accuracy_df = "],"metadata":{"id":"R3faSkS_MeG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Accuracy_df_sorted = "],"metadata":{"id":"Iw7BPkLXMibC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confusion matrix and evaluation metrics for the best ANN model"],"metadata":{"id":"U9iQnVHLqhw9"}},{"cell_type":"code","source":[],"metadata":{"id":"PDU5uRTf3LTj"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WsGTttiM2y9pNEFCnP-bbBr-BTuFEtk8","timestamp":1679089622944}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}