{"cells":[{"cell_type":"markdown","metadata":{"id":"lwy-PAUHopvl"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWwdLjHnopvm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7qzTc5fopvm"},"outputs":[],"source":["# Import 'Tensorflow' pakage\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# Check the version of tensorflow\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgFM50ixjXdk"},"outputs":[],"source":["# Check if a GPU(in Google server) is allocated\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSURm6XzqZNp"},"outputs":[],"source":["# Acess to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","# Load Raw Data and Extract Acceleration Data\n","- Generate single array that consists of every acceleration data (normal and abnormal)"],"metadata":{"id":"2Bi1q0lacPkz"}},{"cell_type":"code","source":["NoOfData = 180\n","\n","for i in range(NoOfData):\n","    \n","    temp_path1 = 'https://github.com/Eunseob/purdue_me597/blob/main/ml_tutorial/Dataset/Normal_%d?raw=true'%(i+1)   # File path of temporary normal data\n","    temp_path2 = 'https://github.com/Eunseob/purdue_me597/blob/main/ml_tutorial/Dataset/Abnormal_%d?raw=true'%(i+1) # File path of temporary abnormal data\n","\n","    exec(\"Normal_%d   = pd.read_csv(temp_path1 , sep=',' , header=None)\"%(i+1))\n","    exec(\"Abnormal_%d = pd.read_csv(temp_path2 , sep=',' , header=None)\"%(i+1))"],"metadata":{"id":"eo_GyVXncOQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DataLength = len(Normal_1)\n","\n","AccData_Nor = pd.DataFrame(np.zeros((NoOfData, DataLength)))\n","AccData_Abn = pd.DataFrame(np.zeros((NoOfData, DataLength)))\n","\n","for i in range(NoOfData):\n","  exec(f\"tempNormal   = Normal_{i+1}\")\n","  exec(f\"tempAbnormal = Abnormal_{i+1}\")\n","\n","  AccData_Nor.iloc[i,:] = tempNormal.iloc[:,1]\n","  AccData_Abn.iloc[i,:] = tempAbnormal.iloc[:,1]\n","\n","AccData = np.array(pd.concat([AccData_Nor, AccData_Abn], axis=0))\n","AccData.shape"],"metadata":{"id":"uR8DCo-2cOKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convert Acceleration Data into Spectrogram by STFT"],"metadata":{"id":"nsWTe-KrxQQ6"}},{"cell_type":"markdown","source":["[Tip] \n","\n","You can define the size of spectrogram (resolution of time and frequency)\n","\n","by adjusting 'Number of samples(N) per segment (nperseg)' and 'Number of samples(N) for overlap'"],"metadata":{"id":"HqdlupjHxZVB"}},{"cell_type":"code","source":["from scipy import signal\n","\n","Fs = 12800  # Sampling Frequency\n","f,t,AccSTFT = signal.spectrogram(AccData, Fs, nperseg = 78, noverlap = 10)\n","AccSTFT.shape"],"metadata":{"id":"ocnP5QWzcOB-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare spectrograms between normal and abnormal"],"metadata":{"id":"0qtw4xnOyCPG"}},{"cell_type":"code","source":["idx = 1  # Select index (1~180)\n","\n","plt.figure(figsize=(12,4))\n","\n","plt.subplot(1,2,1)\n","plt.pcolormesh(t, f, AccSTFT[idx-1], cmap='jet')\n","plt.title(f\"STFT (Normal_{idx})\", fontsize=15)\n","plt.xlabel('Time(s)', fontsize=12)\n","plt.ylabel('Frequency(Hz)', fontsize=12)\n","plt.colorbar()\n","\n","plt.subplot(1,2,2)\n","plt.pcolormesh(t, f, AccSTFT[idx+NoOfData-1], cmap='jet')\n","plt.title(f\"STFT (Abnormal_{idx})\", fontsize=15)\n","plt.xlabel('Time(s)', fontsize=12)\n","plt.colorbar()\n","\n","plt.show()"],"metadata":{"id":"OWHAMw_1cN_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zps3qPZH0CZN"},"source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n","## Split Training & Test Data\n","- Use 'train_test_split' function\n","- It randomly samples the training and testing data according to the designated ratio."]},{"cell_type":"code","source":["NormalSet   = AccSTFT[:NoOfData]\n","AbnormalSet = AccSTFT[NoOfData:]\n","\n","NoOfSensor  = 1\n","NormalSet   = NormalSet.reshape(NormalSet.shape[0], NormalSet.shape[1], NormalSet.shape[2], NoOfSensor)\n","AbnormalSet = AbnormalSet.reshape(AbnormalSet.shape[0], AbnormalSet.shape[1], AbnormalSet.shape[2], NoOfSensor)\n","\n","NormalSet.shape, AbnormalSet.shape"],"metadata":{"id":"sSHeYisJX8SZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection    import train_test_split\n","\n","# Designate test data ratio\n","TestData_Ratio = 0.2 \n","\n","TrainData_Nor, TestData_Nor = train_test_split(NormalSet  , test_size=TestData_Ratio, random_state=777)\n","TrainData_Abn, TestData_Abn = train_test_split(AbnormalSet, test_size=TestData_Ratio, random_state=777)\n","\n","print(TrainData_Nor.shape, TestData_Nor.shape)\n","print(TrainData_Abn.shape, TestData_Abn.shape)"],"metadata":{"id":"T_-DpjKpX7-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Labling (One-hot Encoding)\n","- Use 'np.zeros' and 'np.ones'\n","- '[1,0]' refers to 'Normal' and '[1,0]' refers to 'Abnormal' in this tutorial"],"metadata":{"id":"VlO_u2CjlvZY"}},{"cell_type":"code","source":["TrainLabel_Nor = np.zeros((TrainData_Nor.shape[0],2))\n","TrainLabel_Abn = np.ones( (TrainData_Abn.shape[0],2)) \n","TestLabel_Nor  = np.zeros((TestData_Nor.shape[0],2))\n","TestLabel_Abn  = np.ones( (TestData_Abn.shape[0],2)) \n","\n","TrainLabel_Nor[:,0] = 1  # [1,0]: Normal\n","TrainLabel_Abn[:,0] = 0  # [0,1]: Abnormal\n","TestLabel_Nor[:,0]  = 1  # [1,0]: Normal\n","TestLabel_Abn[:,0]  = 0  # [0,1]: Abnormal\n","\n","print(TrainLabel_Nor.shape, TestLabel_Nor.shape)\n","print(TrainLabel_Abn.shape, TestLabel_Abn.shape)"],"metadata":{"id":"j4-2rzxQboWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data and Label Preparation"],"metadata":{"id":"SR24Ptr1l9mY"}},{"cell_type":"code","source":["TrainData  = np.concatenate([TrainData_Nor , TrainData_Abn ], axis=0)\n","TestData   = np.concatenate([TestData_Nor  , TestData_Abn  ], axis=0)\n","TrainLabel = np.concatenate([TrainLabel_Nor, TrainLabel_Abn], axis=0)\n","TestLabel  = np.concatenate([TestLabel_Nor , TestLabel_Abn ], axis=0)\n","\n","print(TrainData.shape,  TestData.shape)\n","print(TrainLabel.shape, TestLabel.shape)"],"metadata":{"id":"HnZh6FJKX7Bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[".\n","\n",".\n","\n",".\n","\n",".\n","\n","."],"metadata":{"id":"Ea-DpZP9TcaQ"}},{"cell_type":"markdown","metadata":{"id":"TwgRfDG6opvn"},"source":["## Setting hyperparameters for training CNN(Convolutional Neural Network) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBYZBbydopvn"},"outputs":[],"source":["learningRate  = 0.0001\n","Epoch         = 2000"]},{"cell_type":"markdown","metadata":{"id":"75cWnGCIopvn"},"source":["## Designing an CNN architecture (based on Keras)"]},{"cell_type":"markdown","metadata":{"id":"9yCi-yvzopvn"},"source":["- Types of Convolution layer: https://keras.io/api/layers/convolution_layers/\n","\n","- Types of Pooling layer: https://keras.io/api/layers/pooling_layers/\n","\n","- Flatten layer: https://keras.io/api/layers/reshaping_layers/flatten/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZvrun9Oopvo"},"outputs":[],"source":["def CNN_model(input_data):\n","    keras.backend.clear_session()\n","\n","    model = keras.Sequential()\n","    model.add(keras.layers.InputLayer(input_shape=(input_data.shape[1],input_data.shape[2],input_data.shape[3])))       # Input layer\n","\n","    model.add(keras.layers.Conv2D(filters = 2, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))    # Convolution layer 1\n","    model.add(keras.layers.MaxPooling2D(pool_size = (2,2), strides=(2,2)))                                              # Pooling layer 1\n","    model.add(keras.layers.Conv2D(filters = 4, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))    # Convolution layer 2\n","    model.add(keras.layers.MaxPooling2D(pool_size = (2,2), strides=(2,2)))                                              # Pooling layer 2 \n","\n","    model.add(keras.layers.Flatten())                                                                                   # Flatten layer\n","    model.add(keras.layers.Dense(units = 10, activation='relu'))                                                        # Dense layer\n","\n","    model.add(keras.layers.Dense(units = 2, activation='softmax'))                                                      # Output Layer\n","\n","    model.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate),\n","                  loss=keras.losses.CategoricalCrossentropy(),\n","                  metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWxSRDntopvo"},"outputs":[],"source":["# Check the model architecture and the number of parameters\n","CnnModel = CNN_model(TrainData)\n","CnnModel.summary()"]},{"cell_type":"markdown","metadata":{"id":"gISsVxFAopvo"},"source":["## CNN Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SNLBAx1opvo"},"outputs":[],"source":["tf.random.set_seed(777) # Not necessarily required\n","\n","# Model traning and validation\n","TraingHistory  = CnnModel.fit(TrainData, TrainLabel, epochs=Epoch, verbose = 1)"]},{"cell_type":"code","source":["# Evaluation result for test data (not trained)\n","Loss, Accuracy = CnnModel.evaluate(TestData,  TestLabel, verbose=0)\n","Loss, Accuracy # The closer the Loss is to 0 and the closer the accuracy is to 1 (100%), the better."],"metadata":{"id":"0B1trRtcZ7js"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AmVqsTnkzlw"},"outputs":[],"source":["# Check the training process (Loss, Accuracy)\n","\n","fig, loss_ax = plt.subplots(figsize=(8,6))\n","acc_ax = loss_ax.twinx()\n","\n","loss_ax.plot(TraingHistory.history['loss'], label='train loss', c = 'tab:red')\n","loss_ax.set_xlabel('epoch', fontsize=15)\n","loss_ax.set_ylabel('loss', fontsize=15)\n","loss_ax.legend(loc='center left', fontsize=12)\n","\n","acc_ax.plot(TraingHistory.history['accuracy'], label='train acc', c = 'tab:blue')\n","acc_ax.set_ylabel('accuracy', fontsize=15)\n","acc_ax.legend(loc='center right', fontsize=12)\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Save ML model (ANN) as a file"],"metadata":{"id":"8WbOlv7YZmFd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgflb5V4opvq"},"outputs":[],"source":["# Unlike SVM or KNN, no 'Joblib' package is needed.\n","\n","CnnModel.save('/content/drive/MyDrive/Colab Notebooks/SavedFiles/ML_Models/CNN_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"YMy5kgyRopvq"},"source":["Load the saved ML model (ANN) and test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-3monSeopvq"},"outputs":[],"source":["LoadedModel = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/SavedFiles/ML_Models/CNN_model.h5')\n","\n","Loss, Accuracy = LoadedModel.evaluate(TestData, TestLabel, verbose=0)\n","print('[Performance of CNN model] \\n')\n","print('Accuracy : {:.2f}%'.format(Accuracy*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1pcU18Gopvq","scrolled":true},"outputs":[],"source":["# Predicted result\n","Predicted = LoadedModel.predict(TestData)\n","\n","# Convert TestLabel and Predicted into vectors to calculate the confusion matrix and evaluation metrics\n","TestLabel_rev = np.argmax(TestLabel, axis=1)\n","Predicted_rev = np.argmax(Predicted, axis=1)\n","\n","# Plot the confusion matrix\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(TestLabel_rev, Predicted_rev)\n","\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, square=True)\n","plt.xlabel(\"Predicted label\")\n","plt.ylabel(\"True label\")\n","plt.title(\"Confusion Matrix of the CNN Model\")\n","plt.show()\n","\n","from sklearn import metrics\n","\n","# Calculate the evaluation metrics\n","accuracy  = metrics.accuracy_score(TestLabel_rev, Predicted_rev)\n","precision = metrics.precision_score(TestLabel_rev, Predicted_rev)\n","recall    = metrics.recall_score(TestLabel_rev, Predicted_rev)\n","f1_score  = metrics.f1_score(TestLabel_rev, Predicted_rev)\n","\n","# Print the evaluation metrics\n","print(\"\\n\\n\")\n","print(f\"CNN Model Evaluation:\\n\")\n","print(f\"Accuracy : {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall   : {recall:.2f}\")\n","print(f\"F1 Score : {f1_score:.2f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}